{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6277ad9",
   "metadata": {},
   "source": [
    "#### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34e506d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eb223f",
   "metadata": {},
   "source": [
    "#### Token Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26a4c622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_embedding(tokens, embedding_matrix):\n",
    "    \"\"\"Mengambil vektor embedding untuk setiap token.\"\"\"\n",
    "    return embedding_matrix[tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8359ed97",
   "metadata": {},
   "source": [
    "#### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027e00a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(seq_len, d_model):\n",
    "    \"\"\"Membuat matriks sinusoidal positional encoding.\"\"\"\n",
    "    positions = np.arange(seq_len)[:, np.newaxis]\n",
    "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "    \n",
    "    pe = np.zeros((seq_len, d_model))\n",
    "    pe[:, 0::2] = np.sin(positions * div_term)\n",
    "    pe[:, 1::2] = np.cos(positions * div_term)\n",
    "    \n",
    "    return pe[np.newaxis, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae98c2c6",
   "metadata": {},
   "source": [
    "#### Scaled Dot Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1a1492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"Menghitung Scaled Dot-Product Attention.\"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "    scores = (Q @ K.transpose(0, 1, 3, 2)) / np.sqrt(d_k)\n",
    "    \n",
    "    if mask is not None:\n",
    "        scores += mask\n",
    "        \n",
    "    # Softmax\n",
    "    attention_weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n",
    "    attention_weights /= np.sum(attention_weights, axis=-1, keepdims=True)\n",
    "    \n",
    "    output = attention_weights @ V\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af05ccd",
   "metadata": {},
   "source": [
    "#### Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6f7e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "    \"\"\"Implementasi Multi-Head Attention.\"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Inisialisasi bobot\n",
    "        self.W_q = np.random.randn(d_model, d_model)\n",
    "        self.W_k = np.random.randn(d_model, d_model)\n",
    "        self.W_v = np.random.randn(d_model, d_model)\n",
    "        self.W_o = np.random.randn(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Proyeksi linear\n",
    "        Q_proj = x @ self.W_q\n",
    "        K_proj = x @ self.W_k\n",
    "        V_proj = x @ self.W_v\n",
    "        \n",
    "        # Reshape dan transpose untuk multi-head\n",
    "        Q = Q_proj.reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(0, 2, 1, 3)\n",
    "        K = K_proj.reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(0, 2, 1, 3)\n",
    "        V = V_proj.reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(0, 2, 1, 3)\n",
    "        \n",
    "        # Jalankan scaled dot-product attention\n",
    "        attention_output, attention_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Concat dan proyeksi akhir\n",
    "        concatenated = attention_output.transpose(0, 2, 1, 3).reshape(batch_size, seq_len, self.d_model)\n",
    "        output_mha = concatenated @ self.W_o\n",
    "        \n",
    "        return output_mha, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5081c46",
   "metadata": {},
   "source": [
    "#### Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14a13948",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork:\n",
    "    \"\"\"Implementasi Feed-Forward Network.\"\"\"\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        # Inisialisasi bobot dan bias\n",
    "        self.W1 = np.random.randn(d_model, d_ff)\n",
    "        self.b1 = np.zeros(d_ff)\n",
    "        self.W2 = np.random.randn(d_ff, d_model)\n",
    "        self.b2 = np.zeros(d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Lapisan pertama dengan aktivasi ReLU\n",
    "        hidden = np.maximum(0, x @ self.W1 + self.b1)\n",
    "        # Lapisan kedua\n",
    "        output = hidden @ self.W2 + self.b2\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aaa6f2",
   "metadata": {},
   "source": [
    "#### Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c88fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization:\n",
    "    \"\"\"Implementasi Layer Normalization.\"\"\"\n",
    "    def __init__(self, epsilon=1e-5):\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(axis=-1, keepdims=True)\n",
    "        std = x.std(axis=-1, keepdims=True)\n",
    "        return (x - mean) / (std + self.epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78794676",
   "metadata": {},
   "source": [
    "#### Causal Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea88eaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[  0. -inf -inf -inf -inf]\n",
      "  [  0.   0. -inf -inf -inf]\n",
      "  [  0.   0.   0. -inf -inf]\n",
      "  [  0.   0.   0.   0. -inf]\n",
      "  [  0.   0.   0.   0.   0.]]]\n"
     ]
    }
   ],
   "source": [
    "def create_causal_mask(size):\n",
    "    \"\"\"Membuat mask segitiga atas untuk mencegah atensi ke token masa depan.\"\"\"\n",
    "    mask = np.triu(np.ones((1, size, size)), k=1).astype(bool)\n",
    "    return np.where(mask, -np.inf, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5154125e",
   "metadata": {},
   "source": [
    "#### Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "676a5be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputLayer:\n",
    "    \"\"\"Implementasi lapisan output akhir.\"\"\"\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        self.projection = np.random.randn(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Proyeksikan semua token ke ukuran kosakata\n",
    "        all_logits = x @ self.projection\n",
    "        \n",
    "        # Ambil logits dari token terakhir untuk prediksi\n",
    "        last_token_logits = all_logits[:, -1, :]\n",
    "        \n",
    "        # Softmax untuk mendapatkan probabilitas\n",
    "        exp_logits = np.exp(last_token_logits - np.max(last_token_logits, axis=-1, keepdims=True))\n",
    "        next_token_probs = exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)\n",
    "        \n",
    "        return all_logits, next_token_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d467d7",
   "metadata": {},
   "source": [
    "#### Run Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3293fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Input Tokens: (1, 5)\n",
      "Shape Output Akhir (setelah FFN): (1, 5, 512)\n",
      "------------------------------\n",
      "Output Casual Masking Untuk Sequence Dengan Panjang 5\n",
      "[[[  0. -inf -inf -inf -inf]\n",
      "  [  0.   0. -inf -inf -inf]\n",
      "  [  0.   0.   0. -inf -inf]\n",
      "  [  0.   0.   0.   0. -inf]\n",
      "  [  0.   0.   0.   0.   0.]]]\n",
      "------------------------------\n",
      "Shape Semua Logits: (1, 5, 1000)\n",
      "Shape Probabilitas Token Berikutnya: (1, 1000)\n",
      "\n",
      "Probabilitas untuk token berikutnya (10 teratas):\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Persiapan Input ---\n",
    "input_tokens = np.array([[10, 25, 150, 5, 8]])\n",
    "batch_size, seq_len = input_tokens.shape\n",
    "\n",
    "# Inisialisasi Semua Komponen Modular ---\n",
    "embedding_matrix = np.random.randn(vocab_size, d_model)\n",
    "mha_layer = MultiHeadAttention(d_model, num_heads)\n",
    "ffn_layer = FeedForwardNetwork(d_model, d_ff)\n",
    "norm1_layer = LayerNormalization()\n",
    "norm2_layer = LayerNormalization()\n",
    "output_layer = OutputLayer(d_model, vocab_size)\n",
    "\n",
    "# Menjalankan Alur Forward Pass (Satu Blok Decoder) ---\n",
    "\n",
    "# Embedding & Positional Encoding\n",
    "x = token_embedding(input_tokens, embedding_matrix)\n",
    "x += positional_encoding(seq_len, d_model)\n",
    "\n",
    "# Causal Mask\n",
    "mask = create_causal_mask(seq_len)\n",
    "\n",
    "# --- Arsitektur Pre-Norm ---\n",
    "# Blok Multi-Head Attention\n",
    "norm_x = norm1_layer.forward(x)\n",
    "attn_output, weights = mha_layer.forward(norm_x, mask)\n",
    "x = x + attn_output  # Residual connection\n",
    "\n",
    "# Blok Feed-Forward\n",
    "norm_x = norm2_layer.forward(x)\n",
    "ffn_output = ffn_layer.forward(norm_x)\n",
    "x = x + ffn_output  # Residual connection\n",
    "\n",
    "# Output\n",
    "final_logits, final_probs = output_layer.forward(x)\n",
    "\n",
    "print(\"Shape Input Tokens:\", input_tokens.shape)\n",
    "print(\"Shape Output Akhir (setelah FFN):\", x.shape)\n",
    "print(\"-\" * 30)\n",
    "print(\"Output Casual Masking Untuk Sequence Dengan Panjang 5\")\n",
    "print(create_causal_mask(size=5))\n",
    "print(\"-\" * 30)\n",
    "print(\"Shape Semua Logits:\", final_logits.shape)\n",
    "print(\"Shape Probabilitas Token Berikutnya:\", final_probs.shape)\n",
    "print(\"\\nProbabilitas untuk token berikutnya (10 teratas):\")\n",
    "print(np.sort(final_probs[0])[::-1][:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
